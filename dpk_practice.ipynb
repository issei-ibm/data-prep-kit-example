{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample codes for Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# sample dataset used in this example\n",
    "# https://huggingface.co/datasets/tyqiangz/multilingual-sentiments\n",
    "#\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"tyqiangz/multilingual-sentiments\"\n",
    "data = load_dataset(dataset_path, \"all\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = data[\"train\"]\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "for row in itertools.islice(train, 5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Convert datasets into Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "original_data_path = os.path.join(dataset_path, \"original\")\n",
    "!mkdir -p {original_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    data[d].to_parquet(os.path.join(original_data_path, d + \".parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l {original_data_path}\n",
    "\n",
    "from pandas import read_parquet\n",
    "d = read_parquet(os.path.join(original_data_path, \"test.parquet\"))\n",
    "d.to_json(\"test.json\", orient=\"records\")\n",
    "d.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-1: Apply Language Identification with Transform Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please replace c with your credential (access token).\n",
    "#\n",
    "# The following page gives an instruction to generate tokens.\n",
    "# https://huggingface.co/docs/hub/security-tokens\n",
    "#\n",
    "c = \"YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang_id_output_path = os.path.join(dataset_path, \"lang_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dpk_lang_id.transform_python import LangId\n",
    "\n",
    "lang_id_transform = LangId(input_folder=original_data_path,\n",
    "                           output_folder=lang_id_output_path,\n",
    "                           lang_id_model_credential=c,\n",
    "                           lang_id_model_kind=\"fasttext\",\n",
    "                           lang_id_model_url=\"facebook/fasttext-language-identification\",\n",
    "                           lang_id_content_column_name=\"text\")\n",
    "\n",
    "lang_id_transform.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_parquet\n",
    "d = read_parquet(os.path.join(lang_id_output_path, \"train.parquet\"))\n",
    "print(d.loc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3-2: Apply Language Identification with Transform API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dpk_lang_id.transform import (\n",
    "    LangIdentificationTransform,\n",
    "    model_credential_key,\n",
    "    model_kind_key,\n",
    "    model_url_key,\n",
    "    content_column_name_key,\n",
    ")\n",
    "\n",
    "conf = {\n",
    "    model_credential_key: c,\n",
    "    model_kind_key: \"fasttext\",\n",
    "    model_url_key: \"facebook/fasttext-language-identification\",\n",
    "    content_column_name_key: \"text\",\n",
    "}\n",
    "\n",
    "transform = LangIdentificationTransform(conf)\n",
    "\n",
    "from data_processing.data_access import DataAccessLocal\n",
    "data_access = DataAccessLocal()\n",
    "\n",
    "import glob\n",
    "for p in glob.glob(os.path.join(original_data_path, \"*.parquet\")):\n",
    "    print(p)\n",
    "    table, _ = data_access.get_table(p)\n",
    "    #print(type(table))\n",
    "    table, metadata = transform.transform(table)\n",
    "    data_access.save_table(os.path.join(lang_id_output_path, os.path.basename(p)), table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = read_parquet(os.path.join(lang_id_output_path, \"train.parquet\"))\n",
    "print(d.loc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4-1: Apply Filter to choose English texts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_output_path = os.path.join(dataset_path, \"filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dpk_filter.runtime import Filter\n",
    "\n",
    "filter_criteria = [\n",
    "    \"lang = 'en'\",\n",
    "    \"score >= 0.9\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "\n",
    "Filter(input_folder=lang_id_output_path,\n",
    "       output_folder=filter_output_path,\n",
    "       filter_criteria_list=filter_criteria,\n",
    "       filter_logical_operator=filter_logical_operator).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = read_parquet(os.path.join(filter_output_path, \"train.parquet\"))\n",
    "print(d.loc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4-2: Apply Filter with Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dpk_filter.ray.runtime import Filter\n",
    "\n",
    "filter_criteria = [\n",
    "    \"lang = 'en'\",\n",
    "    \"score >= 0.9\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "\n",
    "Filter(input_folder=lang_id_output_path,\n",
    "       output_folder=filter_output_path,\n",
    "       filter_criteria_list=filter_criteria,\n",
    "       filter_logical_operator=filter_logical_operator,\n",
    "       runtime_num_workers=2,\n",
    "       run_locally=True).transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = read_parquet(os.path.join(filter_output_path, \"train.parquet\"))\n",
    "print(d.loc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Your Transform with Transform API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# use importlib to reflect updates in your code to this notebook immediately\n",
    "#\n",
    "import my_transform\n",
    "import importlib\n",
    "importlib.reload(my_transform)\n",
    "\n",
    "from my_transform import (\n",
    "    MyTransform,\n",
    "    column_key,\n",
    ")\n",
    "\n",
    "conf = {\n",
    "    column_key: \"text\",\n",
    "}\n",
    "\n",
    "my_transform_output_path = os.path.join(dataset_path, \"my_transform\")\n",
    "\n",
    "transform = MyTransform(conf)\n",
    "\n",
    "from data_processing.data_access import DataAccessLocal\n",
    "data_access = DataAccessLocal()\n",
    "\n",
    "import glob\n",
    "for p in glob.glob(os.path.join(filter_output_path, \"*.parquet\")):\n",
    "    print(p)\n",
    "    table, _ = data_access.get_table(p)\n",
    "    table, metadata = transform.transform(table)\n",
    "    data_access.save_table(os.path.join(my_transform_output_path, os.path.basename(p)), table[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = read_parquet(os.path.join(my_transform_output_path, \"train.parquet\"))\n",
    "print(d.loc[0:10])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dpk",
   "name": "common-cu113.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu113:m125"
  },
  "kernelspec": {
   "display_name": "dpk",
   "language": "python",
   "name": "tohoku_univ_dpk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
